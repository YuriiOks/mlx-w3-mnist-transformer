{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìù MNIST Dataset: EDA & ViT Patching Exploration üñºÔ∏è‚û°Ô∏èüß©"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**üéØ Goal:** Perform Exploratory Data Analysis (EDA) on the standard MNIST dataset and implement the core patching + embedding mechanism for a Vision Transformer (ViT).\n",
    "\n",
    "**üöÄ This Notebook Covers:**\n",
    "1.  ‚úÖ Setup & Imports\n",
    "2.  üíæ Loading MNIST Dataset\n",
    "3.  üëÄ Visualizing Sample Digits\n",
    "4.  ‚úÇÔ∏è Implementing Image-to-Patch Function\n",
    "5.  üé® Visualizing Patches\n",
    "6.  üìä Implementing Linear Patch Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports üõ†Ô∏èüêç"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import json\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Essential ML/Data libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F # Often useful\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# Progress Bar\n",
    "from tqdm.notebook import tqdm # Use notebook version for better display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÇ Adding project root to sys.path: /Users/Oks_WORKSPACE/Desktop/DEV/W3_project/mlx-w3-mnist-transformer\n",
      "‚öôÔ∏è  Configuring Backprop Bunch logging...\n",
      "  Logger 'Backprop Bunch' level set to: INFO\n",
      "  Clearing existing handlers...\n",
      "  ‚úÖ Console handler added.\n",
      "  ‚úÖ File handler added: logs/mnist_transformer.log\n",
      "2025-04-28 14:46:18 | Backprop Bunch | INFO     | [logging.py:65] | üéâ Logging system initialized!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Backprop Bunch:üéâ Logging system initialized!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Setup Complete.\n"
     ]
    }
   ],
   "source": [
    "# --- Add project root to sys.path for imports ---\n",
    "# Assuming the notebook is run from the project root directory\n",
    "project_root = Path(os.getcwd()).parent\n",
    "src_path = project_root / 'src'\n",
    "if str(project_root) not in sys.path:\n",
    "    print(f\"üìÇ Adding project root to sys.path: {project_root}\")\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Import custom Vocabulary class and logger (Optional - if needed later, otherwise remove)\n",
    "try:\n",
    "    from utils import logger\n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è Error importing project logger: {e}\")\n",
    "    print(\"   Ensure you are running this notebook from the project root directory.\")\n",
    "    # Fallback logger if utils fails\n",
    "    import logging\n",
    "    logger = logging.getLogger(\"EDA_Notebook\")\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    logger.info(\"Using basic fallback logger.\")\n",
    "\n",
    "print(\"\\n‚úÖ Setup Complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load MNIST Dataset üíæüî¢"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use `torchvision` to easily download and load the MNIST dataset. We'll apply standard normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-28 14:47:49 | Backprop Bunch | INFO     | [562368551.py:14] | üíæ Loading MNIST dataset (will download to '/Users/Oks_WORKSPACE/Desktop/DEV/W3_project/mlx-w3-mnist-transformer/data' if needed)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Backprop Bunch:üíæ Loading MNIST dataset (will download to '/Users/Oks_WORKSPACE/Desktop/DEV/W3_project/mlx-w3-mnist-transformer/data' if needed)...\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9.91M/9.91M [00:01<00:00, 7.24MB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28.9k/28.9k [00:00<00:00, 368kB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.65M/1.65M [00:00<00:00, 2.67MB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4.54k/4.54k [00:00<00:00, 261kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-28 14:47:55 | Backprop Bunch | INFO     | [562368551.py:17] | ‚úÖ MNIST loaded. Train size: 60000, Test size: 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "INFO:Backprop Bunch:‚úÖ MNIST loaded. Train size: 60000, Test size: 10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-28 14:47:55 | Backprop Bunch | INFO     | [562368551.py:26] | üöÄ DataLoaders created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Backprop Bunch:üöÄ DataLoaders created.\n"
     ]
    }
   ],
   "source": [
    "# Define basic transforms\n",
    "# 1. Convert image to PyTorch Tensor\n",
    "# 2. Normalize pixel values using mean and std dev calculated across MNIST dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,)) # MNIST specific mean/std\n",
    "])\n",
    "\n",
    "# Define data path (usually relative to project root)\n",
    "data_path = project_root / 'data'\n",
    "\n",
    "# Load MNIST dataset\n",
    "try:\n",
    "    logger.info(f\"üíæ Loading MNIST dataset (will download to '{data_path}' if needed)...\")\n",
    "    train_dataset = torchvision.datasets.MNIST(root=data_path, train=True, download=True, transform=transform)\n",
    "    test_dataset = torchvision.datasets.MNIST(root=data_path, train=False, download=True, transform=transform)\n",
    "    logger.info(f\"‚úÖ MNIST loaded. Train size: {len(train_dataset)}, Test size: {len(test_dataset)}\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"‚ùå Failed to load MNIST: {e}\", exc_info=True)\n",
    "    train_dataset, test_dataset = None, None\n",
    "\n",
    "# Create data loaders (useful for grabbing batches easily)\n",
    "if train_dataset and test_dataset:\n",
    "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
    "    logger.info(\"üöÄ DataLoaders created.\")\n",
    "else:\n",
    "    train_loader, test_loader = None, None\n",
    "    logger.warning(\"Could not create DataLoaders as dataset failed to load.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visualize Sample Digits üëÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to display images\n",
    "def imshow(img_tensor, title=''):\n",
    "    # Reverse the normalization for display purposes\n",
    "    mean = torch.tensor([0.1307])\n",
    "    std = torch.tensor([0.3081])\n",
    "    img_tensor = img_tensor * std[:, None, None] + mean[:, None, None]\n",
    "    \n",
    "    # Clamp values to [0, 1] range\n",
    "    img_tensor = torch.clamp(img_tensor, 0, 1)\n",
    "    \n",
    "    # Convert to numpy for plotting\n",
    "    npimg = img_tensor.numpy()\n",
    "    \n",
    "    # Plot\n",
    "    plt.imshow(np.squeeze(npimg), cmap='gray') # Use np.squeeze for grayscale\n",
    "    plt.title(title)\n",
    "    plt.axis('off') # Hide axes for cleaner look\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a batch of training images\n",
    "sample_image = None # Initialize in case loader failed\n",
    "sample_label = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-28 14:48:26 | Backprop Bunch | INFO     | [1436535450.py:6] | üëÄ Displaying sample MNIST digits:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Backprop Bunch:üëÄ Displaying sample MNIST digits:\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x400 with 8 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-28 14:48:26 | Backprop Bunch | INFO     | [1436535450.py:21] | üíæ Selected one image (Label: 3) for patching demonstration.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Backprop Bunch:üíæ Selected one image (Label: 3) for patching demonstration.\n"
     ]
    }
   ],
   "source": [
    "if train_loader:\n",
    "    dataiter = iter(train_loader)\n",
    "    images, labels = next(dataiter)\n",
    "\n",
    "    # Show images\n",
    "    logger.info(\"üëÄ Displaying sample MNIST digits:\")\n",
    "    num_images_to_show = 8\n",
    "    plt.figure(figsize=(12, 4)) # Adjust figure size \n",
    "    for i in range(num_images_to_show):\n",
    "        if i >= len(images):\n",
    "             break # Handle smaller batches\n",
    "        plt.subplot(1, num_images_to_show, i+1)\n",
    "        imshow(images[i], title=f\"Label: {labels[i].item()}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Keep one image for the patching example\n",
    "    if len(images) > 0:\n",
    "        sample_image = images[0]\n",
    "        sample_label = labels[0]\n",
    "        logger.info(f\"üíæ Selected one image (Label: {sample_label.item()}) for patching demonstration.\")\n",
    "else:\n",
    "    logger.warning(\"ü§∑ Train loader not available, cannot visualize samples or select one for patching.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Patching Implementation ‚úÇÔ∏èüß©"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the core idea of ViT: splitting the image into non-overlapping patches. For a 28x28 MNIST image and a 7x7 patch size, we expect (28/7) x (28/7) = 4x4 = 16 patches. Each patch will be 7x7 pixels (and 1 channel for grayscale)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_to_patches(image_tensor: torch.Tensor, patch_size: int = 7) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Splits a single image tensor (C, H, W) into a sequence of flattened patches.\n",
    "    Uses torch.Tensor.unfold for efficient patching.\n",
    "\n",
    "    Args:\n",
    "        image_tensor (torch.Tensor): Input image tensor (e.g., 1x28x28 for MNIST).\n",
    "        patch_size (int): The height and width of each square patch.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Tensor of flattened patches (NumPatches, PatchDim).\n",
    "                    Returns empty tensor if input is invalid.\n",
    "    \"\"\"\n",
    "    # Basic validation\n",
    "    if not isinstance(image_tensor, torch.Tensor) or image_tensor.ndim != 3:\n",
    "        logger.error(f\"‚ùå Invalid input: Expected a 3D tensor (C, H, W), got {image_tensor.shape if isinstance(image_tensor, torch.Tensor) else type(image_tensor)}\")\n",
    "        return torch.empty(0)\n",
    "        \n",
    "    c, h, w = image_tensor.shape\n",
    "    if h % patch_size != 0 or w % patch_size != 0:\n",
    "        logger.error(f\"‚ùå Image dimensions ({h}x{w}) must be divisible by patch_size ({patch_size}).\")\n",
    "        return torch.empty(0)\n",
    "\n",
    "    num_patches_h = h // patch_size\n",
    "    num_patches_w = w // patch_size\n",
    "    num_patches = num_patches_h * num_patches_w\n",
    "    patch_dim = c * patch_size * patch_size\n",
    "\n",
    "    logger.debug(f\"Input image shape: {image_tensor.shape}\")\n",
    "    logger.debug(f\"Patch size: {patch_size}x{patch_size}\")\n",
    "    logger.debug(f\"Calculated num_patches: {num_patches} ({num_patches_h}x{num_patches_w})\")\n",
    "    logger.debug(f\"Calculated patch_dim (flattened): {patch_dim}\")\n",
    "\n",
    "    try:\n",
    "        # Use unfold to extract patches. unfold(dimension, size, step)\n",
    "        # Unfold height, then width\n",
    "        patches = image_tensor.unfold(1, patch_size, patch_size).unfold(2, patch_size, patch_size)\n",
    "        # Current shape: (C, NumPatchesH, NumPatchesW, PatchH, PatchW)\n",
    "        logger.debug(f\"Shape after unfold: {patches.shape}\")\n",
    "\n",
    "        # Permute and reshape to get (NumPatches, C, PatchH, PatchW)\n",
    "        patches = patches.permute(1, 2, 0, 3, 4).contiguous()\n",
    "        # Shape: (NumPatchesH, NumPatchesW, C, PatchH, PatchW)\n",
    "        logger.debug(f\"Shape after permute: {patches.shape}\")\n",
    "        \n",
    "        # Reshape to (NumPatches, PatchDim) by flattening C, PatchH, PatchW\n",
    "        patches = patches.view(num_patches, patch_dim)\n",
    "        # Shape: (NumPatches, C * PatchH * PatchW)\n",
    "\n",
    "        logger.debug(f\"‚úÖ Output patches shape: {patches.shape}\") # Should be (16, 49) for MNIST 7x7\n",
    "        return patches\n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Error during patching: {e}\", exc_info=True)\n",
    "        return torch.empty(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-28 14:48:44 | Backprop Bunch | INFO     | [2428247436.py:4] | ‚öôÔ∏è Testing image_to_patches function...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Backprop Bunch:‚öôÔ∏è Testing image_to_patches function...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-28 14:48:44 | Backprop Bunch | INFO     | [2428247436.py:7] | ‚úÖ Successfully created patches tensor with shape: torch.Size([16, 49])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Backprop Bunch:‚úÖ Successfully created patches tensor with shape: torch.Size([16, 49])\n"
     ]
    }
   ],
   "source": [
    "# --- Test the function --- \n",
    "patches_tensor = None # Initialize\n",
    "if sample_image is not None:\n",
    "    logger.info(\"‚öôÔ∏è Testing image_to_patches function...\")\n",
    "    patches_tensor = image_to_patches(sample_image, patch_size=7)\n",
    "    if patches_tensor.nelement() > 0: # Check if tensor is not empty\n",
    "        logger.info(f\"‚úÖ Successfully created patches tensor with shape: {patches_tensor.shape}\")\n",
    "    else:\n",
    "        logger.warning(\"‚ö†Ô∏è Patch tensor is empty, likely due to previous errors.\")\n",
    "else:\n",
    "     logger.warning(\"ü§∑ No sample image available to test patching.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize Patches üé®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-28 14:48:49 | Backprop Bunch | INFO     | [3373689613.py:2] | üé® Visualizing the 16 generated patches (reshaped):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Backprop Bunch:üé® Visualizing the 16 generated patches (reshaped):\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x450 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if patches_tensor is not None and patches_tensor.nelement() > 0:\n",
    "    logger.info(f\"üé® Visualizing the {patches_tensor.shape[0]} generated patches (reshaped):\")\n",
    "    try:\n",
    "        num_patches = patches_tensor.shape[0]\n",
    "        patch_dim = patches_tensor.shape[1]\n",
    "        # Assuming square patches and single channel (MNIST)\n",
    "        patch_size = int(patch_dim**0.5) \n",
    "        num_patches_side = int(num_patches**0.5) \n",
    "        \n",
    "        if num_patches_side * num_patches_side != num_patches or patch_size * patch_size != patch_dim:\n",
    "             raise ValueError(\"Cannot determine square layout for patches.\")\n",
    "\n",
    "        plt.figure(figsize=(num_patches_side, num_patches_side + 0.5)) # Adjust size\n",
    "        plt.suptitle(f\"Visualized Patches for Digit '{sample_label.item()}'\", y=1.02)\n",
    "        for i, patch_flat in enumerate(patches_tensor):\n",
    "            plt.subplot(num_patches_side, num_patches_side, i + 1)\n",
    "            # Reshape flattened patch back to image format (1, P, P)\n",
    "            patch_img = patch_flat.view(1, patch_size, patch_size)\n",
    "            \n",
    "            # Display the patch (no need to unnormalize, as we took it *before* imshow unnormalized)\n",
    "            plt.imshow(patch_img.squeeze().numpy(), cmap='gray')\n",
    "            plt.axis('off')\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.98]) # Adjust layout\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Failed to visualize patches: {e}\")\n",
    "else:\n",
    "    logger.warning(\"ü§∑ Patch tensor not available or empty for visualization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Linear Patch Embedding üìä‚ú®"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we project each flattened patch vector (size 7x7 = 49) into a higher-dimensional embedding space (e.g., 64 dimensions) using a simple Linear layer. This is the first step in preparing the patches for the Transformer Encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-28 14:48:57 | Backprop Bunch | INFO     | [957879427.py:6] | ‚öôÔ∏è Creating Linear layer to embed patches: 49 -> 64 dimensions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Backprop Bunch:‚öôÔ∏è Creating Linear layer to embed patches: 49 -> 64 dimensions.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-28 14:48:57 | Backprop Bunch | INFO     | [957879427.py:10] | Embedding Layer: Linear(in_features=49, out_features=64, bias=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Backprop Bunch:Embedding Layer: Linear(in_features=49, out_features=64, bias=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-28 14:48:57 | Backprop Bunch | INFO     | [957879427.py:16] | ‚úÖ Shape after embedding: torch.Size([1, 16, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Backprop Bunch:‚úÖ Shape after embedding: torch.Size([1, 16, 64])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üî¨ Sample embedding vector (first patch, first 10 values):\n",
      "[ 0.24040817  0.10694408  0.09202305  0.25400162 -0.07495159  0.2716779\n",
      " -0.19218512 -0.50649714 -0.03054794  0.0562887 ]\n"
     ]
    }
   ],
   "source": [
    "if patches_tensor is not None and patches_tensor.nelement() > 0:\n",
    "    patch_dim = patches_tensor.shape[1] # Should be 49\n",
    "    embedding_dim = 64 # Example embedding dim from our ViT plan\n",
    "    num_patches = patches_tensor.shape[0] # Should be 16\n",
    "    \n",
    "    logger.info(f\"‚öôÔ∏è Creating Linear layer to embed patches: {patch_dim} -> {embedding_dim} dimensions.\")\n",
    "\n",
    "    # Create the embedding layer\n",
    "    patch_embed_layer = nn.Linear(patch_dim, embedding_dim)\n",
    "    logger.info(f\"Embedding Layer: {patch_embed_layer}\")\n",
    "\n",
    "    # Embed the patches. Input shape for Linear: (Batch, *, InFeatures)\n",
    "    # Our patches_tensor is (NumPatches, PatchDim). Add a batch dim [1, N, P_dim].\n",
    "    try:\n",
    "        patch_embeddings = patch_embed_layer(patches_tensor.unsqueeze(0)) # Add batch dim\n",
    "        logger.info(f\"‚úÖ Shape after embedding: {patch_embeddings.shape}\") # Should be (1, 16, 64)\n",
    "\n",
    "        print(\"\\nüî¨ Sample embedding vector (first patch, first 10 values):\")\n",
    "        print(patch_embeddings[0, 0, :10].detach().numpy()) # Print embedding of the first patch\n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Error during embedding: {e}\", exc_info=True)\n",
    "        \n",
    "else:\n",
    "    logger.warning(\"ü§∑ Patch tensor not available or empty for embedding test.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusion & Next Steps ‚úÖüöÄ\n",
    "\n",
    "We have successfully:\n",
    "*   Loaded the MNIST dataset.\n",
    "*   Visualized sample digits.\n",
    "*   Implemented a function to split images into patches.\n",
    "*   Visualized the resulting patches.\n",
    "*   Implemented a linear layer to create patch embeddings.\n",
    "\n",
    "This notebook confirms our understanding of the initial data processing steps for ViT. The next steps involve building the full ViT architecture (`PatchEmbedding` class, Positional Embeddings, Transformer Encoder blocks, MLP Head) in the `src/` directory."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
