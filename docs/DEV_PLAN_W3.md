# Week 3 Development Plan: MNIST Vision Transformer (MLX Refactor) 🎯📅

**Team:** Backprop Bunch 🎉 (Yurii, Amy, Guillaume, Aygun)
**Goal:** Implement and train a Vision Transformer (ViT) for MNIST digit recognition.

---
**Overall Plan:**

1.  ✅ **Phase 1 (PyTorch):** Single MNIST Digit Classification (28x28).
2.  ✅ **Phase 2 (PyTorch):** 4 Digits in 2x2 Grid Recognition (56x56).
3.  ➡️ **Setup (MLX):** Prepare environment for MLX development.
4.  ➡️ **Phase 1 (MLX):** Reimplement Phase 1 ViT and training loop in MLX.
5.  ➡️ **Deployment:** Create Streamlit UI & Dockerize the MLX application.
6.  ➡️ **(Stretch)** **Phase 2 (MLX):** Adapt MLX implementation for Phase 2 task.
7.  ➡️ **(Stretch)** **Phase 3 (MLX):** Implement dynamic layout recognition in MLX.

---
**Detailed Action Plan Log & To-Do List:**

| Day         | Time Block  | Phase         | Focus Area           | Specific Action(s)                                                                                                                                                                                                                                   | File(s) Involved                                                                 | Goal / Output ✔️                                                                                                                                                 | Status      | Assignee(s) |
| :---------- | :---------- | :------------ | :------------------- | :--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :------------------------------------------------------------------------------- | :--------------------------------------------------------------------------------------------------------------------------------------------------------------- | :---------- | :---------- |
| **Day 1**   | **PM**      | 0             | 🧹 **Cleanup & Setup** | Delete W2 files/dirs. Create W3 dirs (`src/mnist_transformer`, `models/mnist_vit`). Create/Update `config.yaml` (initial). Update `README.md`, `.gitignore`, `docs/STRUCTURE.MD`.                                                                    | Project Root, Config, Docs, `.gitignore`                                         | Clean project ready for W3. Basic config exists.                                                                                                               | ✅ Done       | Team        |
| Day 1       | PM          | 1 (PyTorch)   | 📓 **EDA & Concepts**  | Create `notebooks/01_...ipynb`. Add imports. Load MNIST dataset via `torchvision`. Visualize sample digits.                                                                                                                                          | `notebooks/01_...ipynb`                                                          | MNIST data loaded & visualized.                                                                                                                                  | ✅ Done       | Yurii       |
| Day 1       | PM          | 1 (PyTorch)   | 📓 **Patching**      | Implement `image_to_patches` func in notebook. Visualize patches. Implement simple `nn.Linear` patch embedding & test. (Optional: Test basic Attention func).                                                                                           | `notebooks/01_...ipynb`                                                          | Patching, embedding concepts validated.                                                                                                                          | ✅ Done       | Yurii       |
| Day 1   | **AM**      | 1 (PyTorch)   | 🏗️ **Dataset (PT)**    | Create `src/mnist_transformer/dataset.py`. Implement MNIST `Dataset` loading via `torchvision` (`get_mnist_dataset`, `get_mnist_transforms`). Add test block.                                                                                      | `src/mnist_transformer/dataset.py`                                               | PyTorch Dataset for standard MNIST ready & tested.                                                                                                             | ✅ Done       | Team        |
| Day 1       | AM/PM       | 1 (PyTorch)   | 🏗️ **Modules (PT)**    | Create `src/mnist_transformer/modules.py`. Implement `PatchEmbedding`, `MLPBlock`, `Attention` (initially wrapping `nn.MHA`), `TransformerEncoderBlock`. Add test block. Refactor `Attention` to use custom `AttentionHead` (scratch MHA). Test again. | `src/mnist_transformer/modules.py`                                               | Core ViT components implemented & tested (MHA from scratch).                                                                                                   | ✅ Done       | Team        |
| Day 1       | PM          | 1 (PyTorch)   | 🏗️ **Model (PT)**      | Create `src/mnist_transformer/model.py`. Implement `VisionTransformer` class using custom modules. Add test block for Phase 1 config.                                                                                                        | `src/mnist_transformer/model.py`                                                 | Full ViT architecture defined & tested for Phase 1.                                                                                                            | ✅ Done       | Team        |
| Day 1   | **AM**      | 1 (PyTorch)   | 🚂 **Trainer (PT)**    | Create `src/mnist_transformer/trainer.py`. Implement `train_epoch`, `evaluate_model`, `train_model` orchestrator. Add test block. Implement W&B logging (batch/epoch). Add Evaluation logic.                                                      | `src/mnist_transformer/trainer.py`                                               | Training/Eval loop logic implemented & tested.                                                                                                                 | ✅ Done       | Team        |
| Day 1       | AM          | 1 (PyTorch)   | 🚂 **Script (PT)**     | Create `scripts/train_mnist_vit.py`. Add arg parsing, config loading, device/W&B setup. Add logic to load data, init model/optimizer/loss, call trainer, save artifacts.                                                                          | `scripts/train_mnist_vit.py`                                                     | Executable training script ready for Phase 1.                                                                                                                  | ✅ Done       | Team        |
| Day 1       | **PM**      | 1 (PyTorch)   | 🔥 **Run & Refine(PT)**| Run Phase 1 training (`--phase 1 --epochs 15`). Monitor W&B. Confirm good accuracy (>98%).                                                                                                                                                    | `config.yaml`, Console, W&B                                                    | Working Phase 1 model with >98% accuracy achieved.                                                                                                             | ✅ Done       | Team        |
| Day 1       | PM          | 2 (PyTorch)   | 🔢 **Phase 2 Data**    | Implement `generate_2x2_grid_image`, `MNISTGridDataset` in `dataset.py`. Update test block to test/visualize grid generation.                                                                                                              | `src/mnist_transformer/dataset.py`                                               | Synthetic 2x2 grid dataset generator implemented & tested.                                                                                                     | ✅ Done       | Team        |
| Day 1   | **AM**      | 2 (PyTorch)   | 🔧 **Phase 2 Model**   | Adapt `model.py` (`VisionTransformer`) to handle `img_size=56`, `num_outputs=4`. Update test block for Phase 2 shape checks.                                                                                                              | `src/mnist_transformer/model.py`                                                 | ViT model adapted for 56x56 input, 4-digit output & tested.                                                                                                    | ✅ Done       | Team        |
| Day 1       | AM          | 2 (PyTorch)   | 📉 **Phase 2 Trainer** | Adapt `trainer.py` (`train_epoch`, `evaluate_model`) loss & accuracy calculation for multi-output (reshape tensors, average/sum loss). Update test block.                                                                                   | `src/mnist_transformer/trainer.py`                                               | Trainer logic adapted for Phase 2 multi-output & tested.                                                                                                       | ✅ Done       | Team        |
| Day 1       | PM          | 2 (PyTorch)   | 🔥 **Phase 2 Train**   | Adapt `train_mnist_vit.py` script to load `MNISTGridDataset` for `--phase 2`. Run Phase 2 training (`--phase 2 --epochs 20`). Analyze results (~95.7% acc).                                                                                       | `scripts/train_mnist_vit.py`, Console, W&B                                     | Phase 2 training successful, good accuracy achieved.                                                                                                           | ✅ Done       | Team        |
| Day 1       | PM          | 3 (Design)    | 🤔 **Phase 3 Design**  | Create `notebooks/03_...ipynb`. Brainstorm Phase 3 approaches. Decide on Grid Classification w/ Empty Class. Outline needed changes.                                                                                                     | `notebooks/03_...ipynb`                                                          | Clear plan for Phase 3 implementation (Approach A).                                                                                                            | ✅ Done       | Yurii       |
| **Day 2**    | PM          | 0 (MLX Setup) | 🐍 **Setup MLX**      | Add `mlx` to `requirements.txt`. Run `pip install -r requirements.txt`. Verify installation (`python -c "import mlx.core as mx; print(mx.default_device())"`). Create `feat/mlx-refactor` branch & merge working PyTorch code to `main`.            | `requirements.txt`, Env, Git                                                     | MLX installed. New branch ready for refactoring. `main` is stable.                                                                                             | ✅ Done       | Team        |
| Day 3   | **AM**      | 1 (MLX Port)  | 🏗️ **Dataset (MLX)**   | **Task:** Implement MLX MNIST data loading. <br> - **Action:** Create `src/mnist_transformer/dataset_mlx.py`. Implement `get_mnist_data_mlx` returning MLX arrays, including normalization using `mlx.core` ops. Add test logic.                 | `src/mnist_transformer/dataset_mlx.py`                                           | Load/preprocess MNIST returning MLX arrays.                                                                                                                    | ⏳ **NEXT** | TBD         |
| Day 3       | AM          | 1 (MLX Port)  | 🏗️ **Modules (MLX)**   | **Task:** Reimplement ViT blocks in MLX. <br> - **Action:** Create `src/mnist_transformer/modules_mlx.py`. Implement MLX versions: `PatchEmbeddingMLX`, `AttentionMLX`, `MLPBlockMLX`, `TransformerEncoderBlockMLX`. Test shapes.           | `src/mnist_transformer/modules_mlx.py`                                             | Core ViT components implemented in MLX.                                                                                                                          | ⏳ To Do     | TBD         |
| Day 3       | **PM**      | 1 (MLX Port)  | 🏗️ **Model (MLX)**     | **Task:** Assemble ViT model using MLX modules. <br> - **Action:** Create `src/mnist_transformer/model_mlx.py`. Implement `VisionTransformerMLX` using MLX components. Handle MLX weight init. Test forward pass.                              | `src/mnist_transformer/model_mlx.py`                                               | Full ViT architecture defined in MLX for Phase 1.                                                                                                              | ⏳ To Do     | TBD         |
| Day 3       | PM          | 1 (MLX Port)  | 🚂 **Trainer (MLX)**   | **Task:** Reimplement training loop in MLX. <br> - **Action:** Create `src/mnist_transformer/trainer_mlx.py`. Implement loss/grad func, epoch loop (`mx.eval`, `optimizer.update`), eval func. Add W&B logging hooks using MLX metrics. | `src/mnist_transformer/trainer_mlx.py`                                             | Training/evaluation loop logic using MLX API.                                                                                                                  | ⏳ To Do     | TBD         |
| Day 3   | **AM**      | 1 (MLX Port)  | 🚂 **Script (MLX)**    | **Task:** Create main MLX training script. <br> - **Action:** Create `scripts/train_mnist_vit_mlx.py`. Adapt args/config. Load MLX data/model. Create MLX optimizer/loss. Call MLX trainer. Implement MLX weight save/load.                 | `scripts/train_mnist_vit_mlx.py`, `config.yaml`                                    | Executable MLX training script for Phase 1.                                                                                                                      | ⏳ To Do     | TBD         |
| Day 3       | AM          | 1 (MLX Run)   | 🔥 **Run (MLX)**       | **Task:** Run initial Phase 1 MLX training. <br> - **Action:** Execute `python scripts/train_mnist_vit_mlx.py --phase 1`. Debug MLX issues. Compare W&B metrics to PyTorch baseline.                                                       | Console Output, W&B Run                                                            | Phase 1 ViT trains successfully using MLX.                                                                                                                     | ⏳ To Do     | Team        |
| **Day 4**       | **PM**      | ---           | 🚢 **Streamlit UI**    | **Task:** Build basic web UI for MLX inference. <br> - **Action:** Create `app/app_mlx.py`. Add upload/draw. Load trained MLX model. Preprocess input (MLX). Run inference. Display prediction.                                           | `app/app_mlx.py`, `requirements.txt`                                                   | Streamlit app for classifying digits using MLX model.                                                                                                          | ⏳ To Do     | TBD         |
| Day 4       | PM          | ---           | 🐳 **Dockerization**   | **Task:** Create Dockerfile for Streamlit/MLX app. <br> - **Action:** Define base image, copy files, install deps (MLX!), set entrypoint. Build & test container.                                                                        | `Dockerfile`                                                                         | Docker image for the inference app.                                                                                                                            | ⏳ To Do     | TBD         |
| Day 4       | EOD         | ---           | 🔄 **Review & Plan**   | **Task:** Review MLX porting, UI, Docker. <br> - **Action:** Discuss issues, commit, push. Plan next steps (MLX Phase 2/3?).                                                                                                           | Git Repository, Team Sync                                                              | Day 6 tasks completed, plan refined.                                                                                                                           | ⏳ To Do     | Team        |
| Day 4  | **Ongoing** | 2/3 (MLX)     | ✨ **MLX Phases 2/3**  | **Task:** (Stretch) Reimplement Phase 2/3 logic using MLX.                                                                                                                                                                            | `src/`, `scripts/`                                                               | Functional Phase 2/3 implementation in MLX.                                                                                                                    | ⏳ Optional  | Team        |
| Day 4      | Ongoing     | ---           | 🛠️ **Refinement**      | **Task:** (Stretch) Apply training tricks, HParam tuning for MLX models.                                                                                                                                                               | `config.yaml`, `src/`, `scripts/`                                                    | Improved performance/stability for MLX models.                                                                                                                 | ⏳ Optional  | Team        |
