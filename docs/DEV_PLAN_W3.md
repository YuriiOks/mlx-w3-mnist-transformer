**Detailed Action Plan: MNIST Vision Transformer (Days 1-4)**

| Day         | Time Block  | Phase | Focus Area           | Specific Action(s)                                                                                                                                                                                                                            | File(s) Involved                                                                 | Goal / Output ‚úîÔ∏è                                                                                                                               |
| :---------- | :---------- | :---- | :------------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | :------------------------------------------------------------------------------- | :--------------------------------------------------------------------------------------------------------------------------------------------- |
| **Day 1**   | **PM**      | 0     | üßπ **Cleanup & Setup** | (DONE) Delete W2 files. (DONE) Create new dirs (`src/mnist_transformer`, `models/mnist_vit`). (DONE) Create/Update `config.yaml`, `README.md`, `.gitignore`, `STRUCTURE.MD`.                                                              | Project Root, `config.yaml`, `README.md`, `.gitignore`, `docs/STRUCTURE.MD`      | Clean project ready for W3. Basic config file exists.                                                                                          |
| Day 1       | PM          | 1     | üìì **EDA & Concepts**  | 1. Create `notebooks/01_mnist_vit_exploration.ipynb`. <br> 2. Add imports (`torch`, `torchvision`, `matplotlib`, project utils). <br> 3. Load MNIST dataset (`torchvision.datasets.MNIST`). <br> 4. Visualize sample digits.                  | `notebooks/01_mnist_vit_exploration.ipynb`                                       | MNIST data loaded & visualized.                                                                                                                  |
| Day 1       | PM          | 1     | üìì **Patching**      | 5. Implement `image_to_patches` function in notebook. <br> 6. Visualize patches from a sample digit. <br> 7. Implement simple `nn.Linear` patch embedding in notebook & test. <br> 8. (Optional) Test basic attention function.        | `notebooks/01_mnist_vit_exploration.ipynb`                                       | Understanding of patching, embedding dimensions confirmed.                                                                                       |
| **Day 2**   | **AM**      | 1     | üèóÔ∏è **Dataset**       | 1. Create `src/mnist_transformer/dataset.py`. <br> 2. Implement MNIST `Dataset` class using `torchvision.datasets.MNIST`. <br> 3. Include standard `transforms.ToTensor()`, `transforms.Normalize()`.                             | `src/mnist_transformer/dataset.py`                                               | PyTorch Dataset for standard MNIST ready.                                                                                                    |
| Day 2       | AM          | 1     | üèóÔ∏è **Modules (Part 1)** | 1. Create `src/mnist_transformer/modules.py`. <br> 2. Implement `PatchEmbedding` class (e.g., using `nn.Conv2d` or `unfold` + `Linear`). <br> 3. Implement `MLPBlock` class (`Linear` -> `GELU` -> `Dropout` -> `Linear` -> `Dropout`). | `src/mnist_transformer/modules.py`                                               | Core non-attention ViT building blocks ready.                                                                                                  |
| Day 2       | **PM**      | 1     | üèóÔ∏è **Modules (Part 2)** | 4. Implement `Attention` class (Multi-Head Self-Attention using `nn.Linear` for QKV, softmax, etc., or `nn.MultiheadAttention`). <br> 5. Implement `TransformerEncoderBlock` class combining MHA, MLP, `LayerNorm`, Residuals.         | `src/mnist_transformer/modules.py`                                               | Complete Transformer Encoder block ready.                                                                                                      |
| Day 2       | PM          | 1     | üèóÔ∏è **Model**         | 1. Create `src/mnist_transformer/model.py`. <br> 2. Implement `VisionTransformer` class. <br> 3. Combine `PatchEmbedding`, CLS Token (`nn.Parameter`), Positional Embedding (`nn.Parameter`), stack of `TransformerEncoderBlock`s, `LayerNorm`, MLP Head (`nn.Linear` for 10 classes). | `src/mnist_transformer/model.py`                                                 | Full ViT architecture for Phase 1 defined.                                                                                                     |
| **Day 3**   | **AM**      | 1     | üöÇ **Trainer**       | 1. Create `src/mnist_transformer/trainer.py`. <br> 2. Implement `train_epoch` function (standard classification loop: get batch, model forward, `nn.CrossEntropyLoss`, backward, step). <br> 3. Implement `train_model` orchestrator function. | `src/mnist_transformer/trainer.py`                                               | Training loop logic ready.                                                                                                                     |
| Day 3       | AM          | 1     | üöÇ **Script & Config** | 1. Create `scripts/train_mnist_vit.py`. <br> 2. Add arg parsing, config loading, setup (device, W&B). <br> 3. Add logic to load dataset, instantiate model, optimizer (`AdamW`), call trainer. <br> 4. Update `config.yaml` with Phase 1 hyperparams (image=28, patch=7, dim, depth, heads, lr, batch, epochs). | `scripts/train_mnist_vit.py`, `config.yaml`                                      | Executable training script ready. Config set for Phase 1.                                                                                        |
| Day 3       | **PM**      | 1     | üî• **Run & Refine**    | 1. Run `python scripts/train_mnist_vit.py`. <br> 2. Debug any errors. <br> 3. Monitor W&B. <br> 4. Add basic augmentations (`RandomAffine`?) to `dataset.py`. <br> 5. Add LR scheduler/warmup (optional). Tune LR/epochs/batch size in `config.yaml` / args to reach target accuracy (>98%). | `config.yaml`, `src/mnist_transformer/dataset.py`, `src/mnist_transformer/trainer.py` | Working Phase 1 model, good baseline accuracy achieved.                                                                                        |
| Day 3       | PM          | 2     | üî¢ **Phase 2 Data**    | 1. In `dataset.py`, create function `generate_2x2_grid_data(num_samples)`: generates 56x56 images by tiling 4 MNIST digits, returns (image, [label1, label2, label3, label4]). <br> 2. Modify `MNISTDataset` or create `MNISTGridDataset` to use this generator. | `src/mnist_transformer/dataset.py`                                               | Synthetic 2x2 grid dataset generator implemented.                                                                                              |
| **Day 4**   | **AM**      | 2     | üîß **Phase 2 Model**   | 1. In `model.py` (`VisionTransformer`), modify `__init__` to accept `image_size=56`. Recalculate `num_patches`. <br> 2. Modify `mlp_head`: change output size to `4 * num_classes`. <br> 3. In `forward`, reshape the head output to `(batch_size, 4, num_classes)`. | `src/mnist_transformer/model.py`                                                 | ViT model adapted for 56x56 input and 4-digit output.                                                                                          |
| Day 4       | AM          | 2     | üìâ **Phase 2 Trainer** | 1. In `trainer.py` (`train_epoch`), adapt loss calculation: <br>   - Get model output `(batch, 4, 10)`. <br>   - Get target labels `(batch, 4)`. <br>   - Calculate `CrossEntropyLoss` for each of the 4 digits (loop or vectorized). <br>   - Average the 4 losses. | `src/mnist_transformer/trainer.py`                                               | Loss calculation adapted for multi-output prediction.                                                                                          |
| Day 4       | **PM**      | 2     | üî• **Phase 2 Train**   | 1. In `train_mnist_vit.py`, add arg/config flag to switch to Phase 2 data/model. <br> 2. Run `python scripts/train_mnist_vit.py --phase 2 ...`. <br> 3. Debug and monitor W&B. Analyze accuracy per digit position.              | `scripts/train_mnist_vit.py`, `config.yaml`                                      | Initial training run for Phase 2 completed. Basic multi-digit recognition working.                                                             |
| Day 4       | PM          | 3     | ü§î **Phase 3 Design**  | 1. Discuss Phase 3 approach: How to handle variable digits? Detection tokens? Modify positional embeddings? Output format (`List[Tuple[Class, Position]]`?). <br> 2. Outline necessary changes to `dataset.py`, `model.py`, `trainer.py`. | Docs / Discussion                                                                | Clear plan/design for tackling Phase 3.                                                                                                        |
| Day 4       | EOD         | üîÑ **Review & Merge**  | Review code for Phases 1 & 2. Ensure clarity, comments. Create Pull Requests for completed feature branches (e.g., `feat/vit-phase1-refine`, `feat/vit-phase2-train`) and merge to `main`/`develop`. | Project `main` branch updated with stable Phase 1/2 code. |

