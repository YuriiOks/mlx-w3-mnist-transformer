# MNIST Digit Classifier (Vision Transformer) - Configuration
# 
# Copyright (c) 2025 Backprop Bunch Team (Yurii, Amy, Guillaume, Aygun)
# Created: 2025-04-28
# Updated: 2025-04-28 # <-- Update this date

# --- General Paths ---
paths:
  model_save_dir: "models/mnist_vit"     # Directory to save trained ViT models
  log_dir: "logs"                        # Directory for log files
  log_file_name: "mnist_vit_train.log"   # Name for the main training log file
  # data_dir: "data" # Optional: if you need to specify where MNIST is stored/downloaded

# --- Dataset Parameters ---
dataset:
  # Phase 1 (Single MNIST)
  image_size: 28
  patch_size: 7         # Results in (28/7) * (28/7) = 4 * 4 = 16 patches
  num_classes: 10       # Digits 0-9
  in_channels: 1        # MNIST is grayscale
  # Phase 2 (Grid - Placeholder)
  # image_size_phase2: 56
  # grid_size: 2 # 2x2 grid
  # Phase 3 (Dynamic - Placeholder)
  # image_size_phase3: 64 # Example
  # max_digits: 5 # Example
  # num_classes_phase3: 11 # 0-9 + empty token

# --- Vision Transformer Model Hyperparameters ---
model:
  # Configuration for a small ViT suitable for Phase 1
  embed_dim: 64        # Embedding dimension (vector size for patches)
  depth: 4             # Number of Transformer Encoder blocks
  num_heads: 4         # Number of attention heads
  mlp_ratio: 2.0       # Ratio of MLP hidden dim to embedding dim (e.g., 2.0 * 64 = 128)
  dropout: 0.1         # Dropout rate for linear layers
  attention_dropout: 0.1 # Dropout rate for attention weights

# --- Training Hyperparameters ---
training:
  # Phase 1
  phase1_epochs: 20             # Number of training epochs for Phase 1
  phase1_batch_size: 256        # Training batch size
  phase1_base_lr: 1e-3          # Base learning rate (before warmup/decay)
  phase1_warmup_epochs: 3       # Number of warmup epochs
  phase1_weight_decay: 0.03     # Weight decay for AdamW

  # Placeholder for Phase 2/3 specific settings if needed later
  # phase2_epochs: 30
  # phase2_batch_size: 128
  # phase2_base_lr: 5e-4
  # phase2_warmup_epochs: 3
  # phase2_weight_decay: 0.05

  # General Optimizer Settings
  optimizer: "AdamW"            # Could be Adam, AdamW, SGD etc.
  # Scheduler settings can be added here if using one across phases

# --- Evaluation Parameters ---
evaluation:
  batch_size: 512              # Batch size for evaluation/testing

# --- Logging Configuration (Can reuse from W2) ---
logging:
  log_level: "INFO"
  log_file_enabled: True
  log_console_enabled: True
  log_max_bytes: 10485760      # 10 MB
  log_backup_count: 5