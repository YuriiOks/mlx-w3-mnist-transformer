# MNIST Digit Classifier (Vision Transformer) - Configuration
# Copyright (c) 2025 Backprop Bunch Team (Yurii, Amy, Guillaume, Aygun)
# Created: 2025-04-28
# Updated: 2025-04-28

# --- General Paths ---
paths:
  model_save_dir: "models/mnist_vit"     # Directory to save trained ViT models
  log_dir: "logs"                        # Directory for log files
  log_file_name: "mnist_vit_train.log"   # Name for the main training log file

# --- Dataset Parameters ---
dataset:
  # Phase 1 (Single MNIST)
  image_size: 28
  patch_size: 7
  num_classes: 10
  in_channels: 1
  # Phase 2 (Grid - Placeholder)
  image_size_phase2: 56 # For reference when adapting model/trainer
  grid_size: 2          # 2x2 grid
  num_outputs_phase2: 4
  # Phase 3 (Dynamic - Placeholder)
  # ...

# --- Vision Transformer Model Hyperparameters (Phase 1 Defaults) ---
model:
  embed_dim: 64
  depth: 4
  num_heads: 4
  mlp_ratio: 2.0
  dropout: 0.1         # Dropout in MLP and final head
  attention_dropout: 0.1 # Dropout in Attention layers

# --- Training Hyperparameters ---
training:
  # Phase 1 Defaults
  phase1_epochs: 15             # Train a bit longer based on previous results
  phase1_batch_size: 256
  phase1_base_lr: 1e-3
  phase1_warmup_epochs: 0       # Start without warmup for simplicity
  phase1_weight_decay: 0.03

  # Placeholder for Phase 2/3
  # phase2_epochs: 30
  # ...

  # General Optimizer Settings
  optimizer: "AdamW"

# --- Evaluation Parameters ---
evaluation:
  batch_size: 512              # Can use larger batch size for evaluation

# --- Logging Configuration ---
logging:
  log_level: "INFO"
  log_file_enabled: True
  log_console_enabled: True
  log_max_bytes: 10485760
  log_backup_count: 5